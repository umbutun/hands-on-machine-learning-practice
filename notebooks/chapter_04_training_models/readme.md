# ğŸ“˜ Chapter 4 â€” Training Models

---

### ğŸ§­ Overview
This notebook explores the **core algorithms** behind linear and polynomial regression, focusing on how optimization and regularization influence model performance.  
Youâ€™ll experiment with gradient descent variants and visualize how models learn over time.

---

### ğŸ—ï¸ Key Topics
- Linear and polynomial regression  
- Batch, stochastic, and mini-batch gradient descent  
- Learning-rate tuning and feature scaling  
- Regularization: Ridge, Lasso, Elastic Net  
- Early stopping for overfitting control  

---

### ğŸ’¡ Notebook Highlights
- Implemented regression models from scratch  
- Compared learning behaviors of different optimizers  
- Demonstrated impact of regularization on biasâ€“variance trade-off  
- Visualized cost-function convergence and learning curves  

---

### ğŸ“Š Preview
<p align="center">
  <img src="../../assets/charts/chapter_04_learning_curves.png" width="65%" alt="Learning Curves Example"/>
</p>

*Figure 1 â€” Typical learning curves showing underfitting and overfitting.*

---

### âš™ï¸ Dependencies
- Python â‰¥ 3.10  
- NumPy â€¢ Pandas â€¢ Matplotlib â€¢ Scikit-Learn  
- Jupyter Notebook  

---

### ğŸ“š Resources
- *Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow* â€” AurÃ©lien GÃ©ron (Ch. 4)  
- [Scikit-Learn Linear Models Documentation](https://scikit-learn.org/stable/modules/linear_model.html)  
- [Gradient Descent Algorithm Explained â€“ Towards Data Science](https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3)  

---

### ğŸ’¬ Quote
> *â€œA model that generalizes well learns patterns, not noise.â€*  
> â€” AurÃ©lien GÃ©ron
