# ğŸ“˜ Chapter 6 â€” Decision Trees


### ğŸ§­ Overview
This notebook covers **Decision Trees**, a simple yet powerful family of models that make hierarchical, interpretable decisions.  
Youâ€™ll learn to visualize trees, tune their complexity, and interpret feature importances.

---

### ğŸ—ï¸ Key Topics
- Gini impurity and entropy  
- Tree construction and information gain  
- Pruning and max-depth constraints  
- Decision Tree Regression  
- Feature importance evaluation  

---

### ğŸ’¡ Notebook Highlights
- Built and visualized classification and regression trees  
- Compared entropy vs Gini criteria  
- Examined overfitting behavior in deep trees  
- Extracted and ranked feature importances  

---

### ğŸ““ Notebook
[Open Notebook](./chapter_06_notebook.ipynb)

---

### ğŸ“Š Preview
<p align="center">
  <img src="../../assets/charts/chapter_06_decision_tree_structure.png" width="65%" alt="Decision Tree Example"/>
</p>

*Figure 1 â€” Example decision tree trained on classification data.*

---

### âš™ï¸ Dependencies
- Python â‰¥ 3.10  
- NumPy â€¢ Pandas â€¢ Matplotlib â€¢ Scikit-Learn â€¢ Graphviz  
- Jupyter Notebook  

---

### ğŸ“š Resources
- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)
- [Scikit-Learn Decision Trees Guide](https://scikit-learn.org/stable/modules/tree.html)  
- [Understanding Gini and Entropy â€“ Medium](https://medium.com/@dtuk81/confusion-matrix-accuracy-precision-recall-f1-score-6358683bde3f)  

---

> _â€œEvery split is a decision; every decision defines the path.â€_
