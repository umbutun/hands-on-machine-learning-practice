# ğŸŒ€ Chapter 7 â€” Dimensionality Reduction


### âˆ‘ Overview
This notebook introduces **Dimensionality Reduction** techniques that simplify high-dimensional datasets without losing critical information.  
Youâ€™ll use linear methods like **PCA** and **Incremental PCA**, and non-linear approaches such as **t-SNE**.

---

### ğŸ”‘ Key Topics
- Curse of dimensionality  
- Principal Component Analysis (PCA)  
- Explained variance ratio  
- Incremental and randomized PCA  
- t-SNE for high-dimensional visualization  

---

### ğŸ”† Notebook Highlights
- Performed PCA for compression and visualization  
- Analyzed variance retention by principal components  
- Compared dimensionality reduction outcomes on MNIST  
- Demonstrated t-SNE clustering of handwritten digits  

---

### ğŸ““ Notebook
[Open Notebook](./chapter_07_notebook.ipynb)

---

### ğŸ“Š Preview
<p align="center">
  <img src="../../assets/charts/chapter_07_tsne_visualization.png" width="65%" alt="t-SNE Visualization"/>
</p>

*Figure 1 â€” t-SNE 2D visualization of MNIST embeddings.*

---

### â›“ï¸ Dependencies
- Python â‰¥ 3.10  
- NumPy â€¢ Pandas â€¢ Matplotlib â€¢ Scikit-Learn  
- Jupyter Notebook  

---

### ğŸ“š Resources
- [Hands-On Machine Learning with Scikit-Learn and PyTorch](https://www.oreilly.com/library/view/hands-on-machine-learning/9798341607972/)
- [Scikit-Learn Decomposition Module](https://scikit-learn.org/stable/modules/decomposition.html)  
- [t-SNE Algorithm Explained â€“ Distill.pub](https://distill.pub/2016/misread-tsne/)  

---

_â€œSimplify the space, not the truth hidden within it.â€_  â€” Data Science Proverb
