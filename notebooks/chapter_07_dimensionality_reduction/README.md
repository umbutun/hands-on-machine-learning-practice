# ğŸŒ€ Chapter 7 â€” Dimensionality Reduction


### âˆ‘ Overview
This notebook introduces **Dimensionality Reduction** techniques that simplify high-dimensional datasets without losing critical information.  
Youâ€™ll use linear methods like **PCA** and **Incremental PCA**, and non-linear approaches such as **t-SNE**.

---

### ğŸ”‘ Key Topics
- Curse of dimensionality  
- Principal Component Analysis (PCA)  
- Explained variance ratio  
- Incremental and randomized PCA  
- t-SNE for high-dimensional visualization  

---

### ğŸ”† Notebook Highlights
- Performed PCA for compression and visualization  
- Analyzed variance retention by principal components  
- Compared dimensionality reduction outcomes on MNIST  
- Demonstrated t-SNE clustering of handwritten digits  

---

### ğŸ”— Quick Links

- ğŸ““ **Open Chapter Notebook:**  
  [Open Notebook](./chapter_07_notebook.ipynb)

- â˜ï¸ **Open in Google Colab:**  
  <a href="https://colab.research.google.com/github/umbutun/hands-on-machine-learning-practice/blob/main/notebooks/chapter_07_dimensionality_reduction/chapter_07_notebook.ipynb" target="_blank">Open in Colab</a>

- ğŸ“š **Back to Hands-On-Machine-Learning-Practice Collection:**<br>
  [â¬… Back to Main Collection](https://github.com/umbutun/hands-on-machine-learning-practice)
  
---

### ğŸ“Š Preview
<p align="center">
  <img src="../../assets/charts/chapter_07_tsne_visualization.png" width="65%" alt="t-SNE Visualization"/>
</p>

*Figure 1 â€” t-SNE 2D visualization of MNIST embeddings.*

---

### â›“ï¸ Dependencies
- Python â‰¥ 3.10  
- NumPy â€¢ Pandas â€¢ Matplotlib â€¢ Scikit-Learn  
- Jupyter Notebook  

---

### ğŸ“š Resources
- [Hands-On Machine Learning with Scikit-Learn and PyTorch](https://www.oreilly.com/library/view/hands-on-machine-learning/9798341607972/)
- [Scikit-Learn Decomposition Module](https://scikit-learn.org/stable/modules/decomposition.html)  
- [t-SNE Algorithm Explained â€“ Distill.pub](https://distill.pub/2016/misread-tsne/)  

---

_â€œSimplify the space, not the truth hidden within it.â€_  â€” Data Science Proverb
