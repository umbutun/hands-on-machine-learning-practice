# ğŸ¤– Hands-On Machine Learning Practice

*A collection of practical notebooks, exercises, and projects from my journey through*  
**ğŸ“˜ â€œHands-On Machine Learning with Scikit-Learn and PyTorchâ€ (AurÃ©lien GÃ©ron, 2025)**

---

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)
[![Jupyter](https://img.shields.io/badge/Notebook-Jupyter-orange.svg)]()
[![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-ML-blue.svg)]()
[![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red.svg)]()
[![Book](https://img.shields.io/badge/Book-Hands--On%20Machine%20Learning%203rd%20Ed.-green.svg)](https://www.oreilly.com/library/view/hands-on-machine-learning/9798341607972/)

---

### ğŸ§­ Overview
This repository documents my **hands-on learning journey** as I work through *Hands-On Machine Learning* by AurÃ©lien GÃ©ron.  
Each chapter folder includes:
- Annotated Jupyter notebooks  
- My own notes, experiments, and reflections  
- Small standalone projects inspired by the chapter content  

> ğŸš€ The goal is to learn deeply by building, testing, and explaining everything from scratch.

---

## ğŸ“š Chapters & Notebooks

### Part I. The Fundamentals of Machine Learning  
| Chapter | Title | Description | Resources |
|---|---|---|---|
| 1 | The Machine Learning Landscape | What is machine learning, its types and challenges, overfitting and underfitting, and model selection. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_01_the_machine_learning_landscape/chapter_01_notebook.ipynb) â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_01_the_machine_learning_landscape/README.md) |
| 2 | End-to-End Machine Learning Project ğŸ  | Full ML workflow on housing data â€” data cleaning, feature engineering, model building. | [ğŸŒ&nbsp;Project&nbsp;Repo](https://github.com/umbutun/real-estate-price-predictor)&nbsp;â€¢&nbsp;<br>[ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_02_end_to_end_project/README.md) |(./notebooks/chapter_02_end_to_end_project/README.md) |(./notebooks/chapter_02_end_to_end_project/README.md) |
| 3 | Classification | Intro to classification using MNIST dataset. Logistic regression, precision/recall, ROC, and multiclass methods. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_03_classification/chapter_03_notebook.ipynb) â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_03_classification/README.md) |
| 4 | Training Models | Linear and polynomial regression, gradient descent variants, regularization, and early stopping. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_04_training_models/chapter_04_notebook.ipynb) â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_04_training_models/README.md) |
| 5 | Decision Trees | Decision tree algorithms, splitting criteria (Gini, entropy), overfitting in trees, and interpretability. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_05_decision_trees/chapter_05_notebook.ipynb) â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_05_decision_trees/README.md) |
| 6 | Ensemble Learning & Random Forests | Combining multiple models (bagging, boosting, stacking, voting) and using Random Forests with feature randomness. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_06_ensemble_learning/chapter_06_notebook.ipynb) â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_06_ensemble_learning/README.md) |
| 7 | Dimensionality Reduction | Techniques like PCA, Kernel PCA, and LLE to reduce feature space without losing significant information. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_07_dimensionality_reduction/chapter_07_notebook.ipynb) â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_07_dimensionality_reduction/README.md) |
| 8 | Unsupervised Learning Techniques | Clustering methods (K-Means, DBSCAN, GMM) and anomaly detection on unlabeled data. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_08_unsupervised_learning/chapter_08_notebook.ipynb) â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_08_unsupervised_learning/README.md) |

### Part II. Neural Networks and Deep Learning  
| Chapter | Title | Description | Resources |
|---|---|---|---|
| 9 | Introduction to Artificial Neural Networks | Basics of neural networks, perceptrons, backpropagation, and building small MLPs. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_09_intro_nn/chapter_09_notebook.ipynb)&nbsp;â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_09_intro_nn/README.md) |
| 10 | Building Neural Networks with PyTorch | Learn PyTorch fundamentals: tensors, autograd, DataLoader, custom modules, and simple models. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_10_pytorch_nn/chapter_10_notebook.ipynb)&nbsp;â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_10_pytorch_nn/README.md) |
| 11 | Training Deep Neural Networks | Training deep networks, initialization, optimizers, batch normalization, and scheduling. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_11_training_deep/chapter_11_notebook.ipynb)&nbsp;â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_11_training_deep/README.md) |
| 12 | Deep Computer Vision Using CNNs | Convolutional neural networks in PyTorch: building, training, and transfer learning. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_12_cnn/chapter_12_notebook.ipynb)&nbsp;â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_12_cnn/README.md) |
| 13 | Processing Sequences Using RNNs and CNNs | Sequence data modeling with RNNs, LSTMs or CNNs for time series and text. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_13_sequence/chapter_13_notebook.ipynb)&nbsp;â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_13_sequence/README.md) |
| 14 | Natural Language Processing with RNNs and Attention | NLP using RNNs plus attention mechanisms for translation, summarization, or generation. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_14_nlp_attention/chapter_14_notebook.ipynb)&nbsp;â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_14_nlp_attention/README.md) |
| 15 | Transformers for NLP and Chatbots | Transformers architecture, self-attention, and building simple chatbots / language models. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_15_transformers/chapter_15_notebook.ipynb)&nbsp;â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_15_transformers/README.md) |
| 16 | Vision and Multimodal Transformers | Transformer models for images and multimodal inputs, mixing text + vision. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_16_multimodal_transformers/chapter_16_notebook.ipynb)&nbsp;â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_16_multimodal_transformers/README.md) |
| 17 | Speeding Up Transformers | Techniques for optimizing transformer training: pruning, quantization, and distillation. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_17_speeding_transformers/chapter_17_notebook.ipynb)&nbsp;â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_17_speeding_transformers/README.md) |
| 18 | Autoencoders, GANs & Diffusion Models | Generative models: autoencoders, variational AEs, GANs, and diffusion-based generation. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_18_generative/chapter_18_notebook.ipynb)&nbsp;â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_18_generative/README.md) |
| 19 | Reinforcement Learning | Fundamentals of reinforcement learning, training agents, and policy-based methods. | [ğŸ““&nbsp;Notebook](./notebooks/chapter_19_rl/chapter_19_notebook.ipynb)&nbsp;â€¢ [ğŸ—’ï¸&nbsp;ReadMe](./notebooks/chapter_19_rl/README.md) |

---

## ğŸ§  Key Learning Goals
- Build, train, and evaluate ML models with Scikit-Learn  
- Design reproducible ML workflows  
- Understand feature engineering and data preprocessing  
- Explore deep learning and neural networks with PyTorch 
- Deploy and scale models for real-world use  

---

## âš™ï¸ Setup & Usage

```bash
# Clone the repository
git clone https://github.com/umbutun/hands-on-machine-learning-practice.git
cd hands-on-machine-learning-practice

# Create environment (if using Conda)
conda env create -f environment.yml
conda activate hands-on-ml

# Or install dependencies via pip
pip install -r requirements.txt

# Launch Jupyter
jupyter notebook
```
---
## ğŸ“ Repo Structure

```plaintext
notebooks/
â””â”€â”€ chapter_01_the_machine_learning_landscape/
â””â”€â”€ chapter_02_end_to_end_project/
â””â”€â”€ chapter_03_classification/
â””â”€â”€ chapter_04_training_models/
â””â”€â”€ chapter_05_decision_trees/
â””â”€â”€ chapter_06_ensemble_learning/
â””â”€â”€ chapter_07_dimensionality_reduction/
â””â”€â”€ chapter_08_unsupervised_learning/
â””â”€â”€ chapter_09_intro_nn/
â””â”€â”€ chapter_10_pytorch_nn/
â””â”€â”€ chapter_11_training_deep/
â””â”€â”€ chapter_12_cnn/
â””â”€â”€ chapter_13_sequence/
â””â”€â”€ chapter_14_nlp_attention/
â””â”€â”€ chapter_15_transformers/
â””â”€â”€ chapter_16_multimodal_transformers/
â””â”€â”€ chapter_17_speeding_transformers/
â””â”€â”€ chapter_18_generative/
â””â”€â”€ chapter_19_rl/

assets/
â””â”€â”€ banners/
â””â”€â”€ charts/
```
---

## ğŸ§© Dependencies
	â€¢	Python 3.10+
	â€¢	Scikit-Learn
	â€¢	NumPy, Pandas, Matplotlib, Seaborn
	â€¢	PyTorch
	â€¢	JupyterLab

(See [requirements.txt](requirements.txt) for full list.)

---

## ğŸ† Highlights
	â€¢	âœ… Modular structure by chapter
	â€¢	âœ… Real projects linked separately (e.g. Real Estate Price Predictorï¿¼)
	â€¢	âœ… Reproducible environments
	â€¢	âœ… Learning-first, project-based approach

---

## ğŸ§‘â€ğŸ’» Author

â€¢ Umut BÃ¼tÃ¼n
ğŸ“ Machine Learning Enthusiast | Python Developer<br>
ğŸ”— [GitHub Profile](https://github.com/umbutun)ï¿¼

---

## ğŸ“œ License

This repository is licensed under the MIT License.

---

> _â€œLearning by doing â€” one model, one notebook at a time.â€_
